{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QVlB0NhSet0-"
      },
      "outputs": [],
      "source": [
        "!pip install pandas\n",
        "!pip install matplotlib\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/content/go_emotions_dataset.csv')\n",
        "data"
      ],
      "metadata": {
        "id": "BOQaZ_maeyNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head() # Top 5 rows of the data set"
      ],
      "metadata": {
        "id": "PY8Z5lojeyKA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.tail() # Last 5 rows of the data set"
      ],
      "metadata": {
        "id": "RGDXJePkeyGl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info() # information of the dat set like , data type , memory usage"
      ],
      "metadata": {
        "id": "_HFHJIMBeyDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.describe() # stastical information of the data set"
      ],
      "metadata": {
        "id": "Nlvz4QXbeyAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#checking the null values of the data set\n",
        "data.isnull().sum()\n"
      ],
      "metadata": {
        "id": "8BPKtHJGex9b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated()"
      ],
      "metadata": {
        "id": "-G5kpD1bex6i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of emotion label columns (from column 3 onward in your dataset)\n",
        "emotion_columns = data.columns[3:]  # assuming first 3 columns are id, text, and unclear flag\n",
        "\n",
        "# Count total occurrences of each emotion\n",
        "value_counts = data[emotion_columns].sum().sort_values(ascending=False)\n",
        "\n",
        "print(value_counts)\n"
      ],
      "metadata": {
        "id": "iqoqqcFOex3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "T1FfQhKrexxl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming 'value_counts' is the Series with emotion counts\n",
        "value_counts = data[emotion_columns].sum().sort_values(ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "value_counts.plot(kind='bar', color='skyblue')\n",
        "\n",
        "plt.title('Emotion Value Counts')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Emotion')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "plt.figure(figsize=(16, 6))  # reasonable size\n",
        "ax = value_counts.plot(kind='bar', color='skyblue')\n",
        "\n",
        "# Add value labels above each bar (optional)\n",
        "for i, value in enumerate(value_counts):\n",
        "    ax.text(i, value + 300, str(int(value)), ha='center', fontsize=8, rotation=90)\n",
        "\n",
        "plt.title('Emotion Value Counts')\n",
        "plt.ylabel('Count')\n",
        "plt.xlabel('Emotion')\n",
        "plt.xticks(rotation=45)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ctL04yTTexuG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wordcloud"
      ],
      "metadata": {
        "id": "kk9G4EtuhjN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We install the `wordcloud` library in Python to generate word cloud visualizations. A word cloud is a graphical representation of text data where the size of each word indicates its frequency or importance. This tool is particularly useful for visualizing the most prominent words in a text corpus, such as in sentiment analysis or text mining projects. It helps in quickly identifying the most common words or themes in the data."
      ],
      "metadata": {
        "id": "ee4DAhUhjBXS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Combine all text data into a single string\n",
        "combined_text = \" \".join(data['text'])\n",
        "\n",
        "# Generate word cloud\n",
        "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.imshow(wordcloud, interpolation='bilinear')\n",
        "plt.axis('off')\n",
        "plt.title('Word Cloud of GoEmotions Text Data')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zqChulhbhjKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter"
      ],
      "metadata": {
        "id": "VTL0ZiUbhjHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define target words\n",
        "target_words = ['food', 'place', 'restaurant']\n",
        "\n",
        "# Combine and tokenize text\n",
        "all_words = \" \".join(data['text']).lower().split()\n",
        "\n",
        "# Count word occurrences\n",
        "word_counts = Counter(all_words)\n",
        "\n",
        "# Extract counts of target words\n",
        "target_word_counts = {word: word_counts[word] for word in target_words}\n",
        "\n",
        "# Plot\n",
        "plt.figure(figsize=(8,6))\n",
        "plt.bar(target_word_counts.keys(), target_word_counts.values(), color=['blue','green','orange'])\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Frequency of Specific Words in Text')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nFbAGKY2hjEq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Text preprocessing"
      ],
      "metadata": {
        "id": "x0RS2HTglRE-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text column to lowercase\n",
        "lowercased_text = data['text'].str.lower()\n",
        "print(lowercased_text)\n"
      ],
      "metadata": {
        "id": "JHR-1e80lNTN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The word_tokenize function from the nltk.tokenize module in the Natural Language Toolkit (NLTK) library is used to split text into individual words, a process known as tokenization. Tokenization is a fundamental step in natural language processing (NLP) tasks, allowing you to work with individual words rather than whole sentences or paragraphs.\n",
        "\n"
      ],
      "metadata": {
        "id": "ubFtFIbSnI7d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "data['Tokens'] = data['text'].apply(tokenizer.tokenize)\n"
      ],
      "metadata": {
        "id": "j29cGNOUlj2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data[['text', 'Tokens']].head())"
      ],
      "metadata": {
        "id": "BDRPhLUYljyl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "data['text'] = data['text'].str.replace(f\"[{string.punctuation}]\",\" \",regex = True)\n",
        "print(data['text'])"
      ],
      "metadata": {
        "id": "_mXjlKeYljvw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The stopwords module from the NLTK library is used to remove common words (like \"this,\" \"is,\" \"are,\" \"was,\" etc.) that do not carry significant meaning and are often removed during text preprocessing in NLP tasks."
      ],
      "metadata": {
        "id": "ZOpkYViqoHLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "# Download only what's needed\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "id": "NJlzBQYrljs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "B9rCXwxqoGu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define tokenizer and stopwords\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "# Apply tokenization + cleaning without using 'punkt'\n",
        "data['Tokens'] = data['text'].apply(\n",
        "    lambda x: [word for word in tokenizer.tokenize(x.lower()) if word.isalpha() and word not in stop_words]\n",
        ")\n",
        "\n",
        "# Preview result\n",
        "print(data[['text', 'Tokens']].head())"
      ],
      "metadata": {
        "id": "0geIWbwrljp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(data['Tokens'])"
      ],
      "metadata": {
        "id": "Ma1jdSDLobtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#stemming\n",
        "stemming is the process of reducing the a word into root or base word form by removig suffix\n",
        "example : driving stemmed is drive"
      ],
      "metadata": {
        "id": "GWijWY2Co3_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Stemming\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "f5hHBlpDobqF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer = PorterStemmer()\n",
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "xHDsYKhuljnH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['stemmed'] = data['text'].apply(\n",
        "    lambda x: ' '.join([stemmer.stem(word) for word in tokenizer.tokenize(x.lower()) if word.isalpha()])\n",
        ")\n",
        "\n",
        "# Preview\n",
        "print(data[['text', 'stemmed']].head())"
      ],
      "metadata": {
        "id": "_ChU0WiQo_eH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lemmatization\n",
        "Lemmatization is the process transforming a word into its base or dictionary form\n",
        "example is better is lemmtized to good"
      ],
      "metadata": {
        "id": "1TwOizFvpMQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "# Download WordNet resources (skip punkt to avoid issues)\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "id": "4s_O1jP5o_aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize tokenizer and lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "tokenizer = TreebankWordTokenizer()"
      ],
      "metadata": {
        "id": "b4BSGqHHo_Xq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lemmatization safely without word_tokenize\n",
        "data['Lemmatized'] = data['text'].apply(\n",
        "    lambda x: ' '.join([lemmatizer.lemmatize(word, pos=wordnet.VERB)\n",
        "                        for word in tokenizer.tokenize(x.lower()) if word.isalpha()])\n",
        ")\n",
        "\n",
        "# Preview the output\n",
        "print(data[['text', 'Lemmatized']].head())"
      ],
      "metadata": {
        "id": "ANKrFf49pWmV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the numbers from reviews\n",
        "import re\n",
        "data['No_Numbers'] = data['text'].apply(lambda x : re.sub(r'\\d+',' ' ,x))\n",
        "print(data['No_Numbers'])"
      ],
      "metadata": {
        "id": "ashlgnqgpWix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#removing special characters like @ # %,*\n",
        "data['cleaned'] = data['text'].apply(lambda x: re.sub(r'[^A-Za-z0-9\\s]','' ,x))\n",
        "print(data['cleaned'])"
      ],
      "metadata": {
        "id": "XUlUqYQQpWf8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#expanding method\n",
        " don't eat food in this hotel , when we apply expanted text it will convert into do not eat food in this hotel"
      ],
      "metadata": {
        "id": "8hdT01KqqLHJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install contractions"
      ],
      "metadata": {
        "id": "1YgZRvxjqK1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import contractions\n",
        "data['Expanded'] = data['text'].apply(contractions.fix)\n",
        "print(data['Expanded'])"
      ],
      "metadata": {
        "id": "BTklaKfRqUk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing emojis\n",
        "!pip install emoji"
      ],
      "metadata": {
        "id": "nyQ3C1kbqY85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import emoji\n",
        "data['emoji'] = data['text'].apply(emoji.demojize)\n",
        "print(data['emoji'])"
      ],
      "metadata": {
        "id": "GVnZ3JQsqira"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# removing liks from review_ text\n",
        "food is good vist www.abchotel.in"
      ],
      "metadata": {
        "id": "olOzqJOwqsBX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install beautifulsoup4"
      ],
      "metadata": {
        "id": "Luml6Qnnqo7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from bs4 import BeautifulSoup"
      ],
      "metadata": {
        "id": "kAb1SuUIq0lV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['cleaned'] = data['text'].apply(lambda x: BeautifulSoup(x,\"html.parser\").get_text())\n",
        "print(data['cleaned'])"
      ],
      "metadata": {
        "id": "F8JPQuyfq0h9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#TF IDF VECTORIZER converts catigorical data into numerical data so thar machine can understand it better\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer = TfidfVectorizer()"
      ],
      "metadata": {
        "id": "4l0WKXScq0e0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = vectorizer.fit_transform(data['text'])\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "id": "BDlMMys3q0cC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#bulding a machine learning model\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n"
      ],
      "metadata": {
        "id": "d-daaw7dra0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify emotion columns (assumes they start from column index 3)\n",
        "emotion_columns = data.columns[3:]\n",
        "\n",
        "# Step 2: Create target label â€” the most active emotion for each row\n",
        "print(data[emotion_columns].dtypes)\n"
      ],
      "metadata": {
        "id": "5J2eDbYyrawv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Identify emotion columns (assumes they start from column index 3 and end before the text processing columns)\n",
        "# Based on the data info and head, the emotion columns are from index 3 up to 'neutral'.\n",
        "emotion_columns = data.columns[data.columns.get_loc('admiration'):data.columns.get_loc('neutral') + 1]\n",
        "\n",
        "# The emotion columns are already in integer format (0 or 1), so no conversion is needed.\n",
        "# The previous error was caused by including text processing columns in emotion_columns.\n",
        "\n",
        "print(data[emotion_columns].dtypes) # You can uncomment this to verify the data types"
      ],
      "metadata": {
        "id": "wKiI_cESUoQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_test_tfidf)"
      ],
      "metadata": {
        "id": "RAWyGcBEsOW1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test,y_pred)"
      ],
      "metadata": {
        "id": "04wS-kl5sONX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "report = classification_report(y_test,y_pred)"
      ],
      "metadata": {
        "id": "YvyvVqiasOJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy {accuracy}')"
      ],
      "metadata": {
        "id": "VGm_INugsY_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Classfication Report:')\n",
        "print(report)"
      ],
      "metadata": {
        "id": "FgcoXz-vsemV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prediction of new review\n",
        "def preprocess_review(review):\n",
        "    review = review.lower()\n",
        "    review = BeautifulSoup(review,\"html.parser\").get_text()\n",
        "    review = re.sub(f\"[{string.punctuation}]\",\" \",review)\n",
        "    review = contractions.fix(review)\n",
        "    review = emoji.demojize(review)\n",
        "    tokens = word_tokenize(review)\n",
        "    stop_words =set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word, pos = 'v') for word in tokens]\n",
        "    cleaned_review = ' '.join(lemmatized_tokens)\n",
        "    return cleaned_review"
      ],
      "metadata": {
        "id": "EQCrItQ2swes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new review prediction\n",
        "new_review = input(\"Enter a review\")\n",
        "cleaned_review = preprocess_review(new_review)\n",
        "new_review_vectorized = vectorizer.transform([cleaned_review])\n",
        "prediction = model.predict(new_review_vectorized)\n",
        "if prediction[0] == 1:\n",
        "    print(\"The review is predicted postive\")\n",
        "else:\n",
        "    print(\"The review is predicted negative\")"
      ],
      "metadata": {
        "id": "dTLtE_3ks2au"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ee10c2e"
      },
      "source": [
        "# Step 3: Create a single target variable by taking the first emotion with a value of 1,\n",
        "# or 'neutral' if all emotion values are 0.\n",
        "def get_most_prominent_emotion(row):\n",
        "    for col in emotion_columns:\n",
        "        if row[col] == 1:\n",
        "            return col\n",
        "    return 'neutral'  # Default to neutral if no emotion is marked with 1\n",
        "\n",
        "data['most_prominent_emotion'] = data.apply(get_most_prominent_emotion, axis=1)\n",
        "\n",
        "# Display the first few rows with the new column\n",
        "print(data[['text', 'most_prominent_emotion']].head())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9be3b27"
      },
      "source": [
        "# Step 4: Split the data into training and testing sets\n",
        "X = data['Lemmatized'] # Using the lemmatized text data as features\n",
        "y = data['most_prominent_emotion'] # Using the most prominent emotion as the target variable\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Testing set size: {X_test.shape[0]}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8c835d46"
      },
      "source": [
        "# Step 5: Vectorize the text data using TF-IDF\n",
        "# The TfidfVectorizer was initialized in a previous cell (cell_id: 4l0WKXScq0e0)\n",
        "\n",
        "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
        "X_test_tfidf = vectorizer.transform(X_test)\n",
        "\n",
        "print(f\"Shape of X_train_tfidf: {X_train_tfidf.shape}\")\n",
        "print(f\"Shape of X_test_tfidf: {X_test_tfidf.shape}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a35b728"
      },
      "source": [
        "# Step 6: Train a Multinomial Naive Bayes model\n",
        "model = MultinomialNB()\n",
        "model.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"Model training complete.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d420246a"
      },
      "source": [
        "# Step 7: Make predictions on the test set\n",
        "y_pred = model.predict(X_test_tfidf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abab1a5e"
      },
      "source": [
        "# Step 8: Evaluate the model\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "report = classification_report(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy}')\n",
        "print('Classification Report:')\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L-pfnc58XkKs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "git remote add origin https://github.com/karthiksuru06/SENTIMENT-ANALYSIS.git\n",
        "git branch -M main\n",
        "git push -u origin main"
      ],
      "metadata": {
        "id": "TiK8HeiRXhoF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}